
```{r lab3_setup, include=FALSE}

knitr::opts_chunk$set(error = TRUE, warning = TRUE, message = TRUE)
suppressPackageStartupMessages({
  if (requireNamespace("tidyverse", quietly = TRUE)) library(tidyverse)
  if (requireNamespace("RcppRoll", quietly = TRUE)) library(RcppRoll)
})

# Data helpers and canonical path to the CO₂ file in this lab
data_dir <- "data"
path_data <- function(...) file.path(data_dir, ...)
co2_path <- path_data("co2_mm_mlo.txt")
```


## Lab: Data manipulation and visualization using the Tidyverse (Student Version)

**Data files**
The lab expects a folder named **data/** next to this Rmd. Files included:

```
data/499_GRN_ANT_mass_changes.csv
data/647_Global_Temperature_Data_File.txt
data/N_seaice_extent_daily_v3.0.csv
data/antarctica_mass_200204_202310.txt
data/co2_mm_mlo.txt
```

### Lesson Overview

This lesson was written by Dr. Carl Boettiger, UC Berkeley. Part of his commitment to open science is to share his teaching materials (Thanks Carl!). Check out his [research group](https://www.carlboettiger.info/)

Part of this lesson is a bit of a scavenger hunt for trying to understand the publicly available data on climate monitoring. Some information on these datasets will be difficult to find, and hopefully inspire detailed meta-data documentation but everyone in this class in the future ;) In previous years, we have downloaded this data from online, but I worried that some data would be taken down so the data is included pre-downloaded.

**Conservation/ecology Topics** 

- Become familiar with the primary data sources and evidence for global warming

**Computational Topics**

- Learn to discover and interpret essential metadata about how measurements are made
- Interpret Data provenance, "Raw" and "Derived" data
- Think about measurement uncertainty, resolution, and missing values in context of environmental science data
- Reading in data from the web into the R.
- Become familiar with variations in CSV / tabular data formats and how to handle them
- Encountering missing data
- Working with dates and date-time objects
- Plotting timeseries data
- Subsetting, reshaping data
- `apply` functions

**Statistical Topics**

- Interpret data visualizations
- Explore noise vs seasonality vs trends
- Understand the use of windowed averages

-------------------------------

### Demo: Evidence for Global Climate Change

In this module, we will explore several of the most significant data sources on global climate change.  An introduction to these data sources can be found at NASA's [Climate Vital Signs website](http://climate.nasa.gov/vital-signs).

 We will begin by examining the carbon dioxide record from the Mauna Loa Observatory.

**Why C02?**

Carbon dioxide (CO2) is an important heat-trapping (greenhouse) gas, which is released through human activities such as deforestation and burning fossil fuels, as well as natural processes such as respiration and volcanic eruptions.

**Parsing tabular data**

One of the most common formats we will interact with is tabular data. Tabular data is often presented in *plain text*, which is not as simple as it sounds, (as we shall see in a moment).  NASA points us to a raw data file maintained by [NOAA on one of it's FTP servers](ftp://aftp.cmdl.noaa.gov/products/trends/co2/co2_mm_mlo.txt).

So, where does this data come from? How does one measure atmospheric CO2 levels anyway?

**Data Provenance**

Knowing where our data come from and how values are measured is essential to proper interpretation of the results.  Data scientists usually speak of *raw data* and *derived data*, but the intepretation of these terms is always relative.  Typically *raw* simply means "the data I started with" and *derived* "the data I produced."  Thus our "raw" data is almost always someone else's "derived" data, and understanding how they got to it can provide important insights to our analysis.  One of the first questions we should ask of any data is "where does it come from?"

In particular, we usually want to make note of three things:

- 1. What is the uncertainty in the data?
- 2. What is the resolution of the data?
- 3. What do missing values mean?

**1.  What is the uncertainty in the data?**

Almost all measurements come with some degree of uncertainty, or measurement error.  Often we will not be able to know this precisely. Rather, we seek a a qualtiative understanding of the measurement process to give us some idea of the relative importance of errors in the measurement process influencing the value.  We may later be able to infer a more precise description of measurement error from the data itself, but this will always require assumptions about both the data-generating process itself.

**2. What is the resolution of the data?**

Derived data often summarize raw data in some way.  For instance, global climate data is frequently reported as monthly or even annual averages, even though the raw data may be collected day by day, or even minute by minute.  Data may be averaged over space as well as time, such as weather measurements made in at separate stations.  Weighted averages and more complex techniques are often used as well.

**3. What do missing values mean?**

Real world data almost always has missing values.  Here, it is important we try to understand *why* values are missing so we know how to handle them appropriately.  If there is a systematic reason behind why data are missing (say, days where snowfall or storms made the weather station inaccessible) they could bias our analysis (underestimating extreme cold days, say).  If data are missing for an unrelated reason (the scientist is sick, or the instrument fails) then we may be more justified in simply ommitting the data.  Often we cannot know the exact reason certain data are missing and this is just something we must keep in mind as a caveat to our infererence. Frequently our results will be independent of missing data, and sometimes missing data can be accurately inferred from the data that is available.

**Measuring C02 levels**

So how *are* atmospheric CO2 levels measured?

Researchers shine an infrared light source of a precise intensity through dry air in a container of precisely controlled volume & pressure, ensuring a consistent number of atoms in the chamber. CO2 absorbs some of this radiation as it passes through the chamber, and then a sensor on the opposite end measures the radiation it receives, allowing researchers to calculate the amount absorbed and infer the CO2 concentration.  The data are reported in parts per million (ppm), a count of the number of CO2 molecules per million molecules of dry air.  These calculations are calibrated by comparing against chambers that are prepared using known concentrations of CO2.  For more information, see [NOAA documentation](http://www.esrl.noaa.gov/gmd/ccgg/about/co2_measurements.html).

**Measurement uncertainty:**

Importantly, the measurement error introduced here is rather small, roughly 0.2 ppm.  As we shall see, many other factors, such as local weather and seasonal variation also influence the measurement, but the measurement process itself is reasonably precise.  As we move to other sources of data these measurement errors can become much more significant.

**Resolution:**

What is the resolution of the CO2 data? Already we see our data are not the actual "raw" measurements the researchers at Mauna Loa read off their instruments each day, but have been reported as monthly averages.

**Missing values:**

The last column of the data set tells us for how many days that month researchers collected data.  We see that they only started keeping track of this information in 1974, but have since been pretty diligent -- collecting data almost every day of the month (no breaks for weekends here!  What do you think accounts for the gaps?  How might you test your hypothesis?  Would these introduce bias to the monthly averages? Would that bias influence your conclusion about rising CO2 levels?)

Spatially our Mauna Loa data has no aggregation -- the data is collected at only one location.  How might the data differ if it were aggregated from stations all over the globe?

**Importing Data**

**Goal.** Load the NOAA Mauna Loa monthly CO₂ file that is bundled with this lab at `data/co2_mm_mlo.txt`.

**Watch‑outs (why parsing is tricky):**
- Lines starting with `#` are **comments** that describe the columns.
- Some numeric columns use **sentinel codes** like `-9.99` to mean “missing” — we’ll convert those to real `NA`.
- The file includes a `decimal_date`, but we’ll also construct a proper **`Date`** from `year` + `month` for plotting.

We’ll first try a naive import (to see the problems), then a fix that **tells R about comments**, and finally a tidyverse‑style read that
**assigns column names**, **converts sentinels to `NA`**, and **adds a `Date` column**.

Our first task is to read this data into our R environment.  To this, we will use the `read.csv` function. Reading in a data file is called *parsing*, which sounds much more sophisticated.  For good reason too -- parsing different data files and formats is a cornerstone of all pratical data science research, and can often be the hardest step.

So what do we need to know about this file in order to read it into R?

```{r, render=print}
### Let's try:
co2.test1 <- read.csv(co2_path)
head(co2.test1)
```

hmm... what a mess.  Let's try defining the comment symbol:
```{r, render=print}
co2.test2 <- read.csv(co2_path,
                 comment = "#")
head(co2.test2)
```

Getting there, but not quite done. Our first row is being interpreted as column names.  The documentation also notes that certain values are used to indicate missing data, which we would be better off converting to explicitly missing so we don't get confused.

Seems like we need a more flexible way to load in the data to avoid further suffering. Let's try `readr::read_table` from `tidyverse` 

```{r}

co2 <- read_table(co2_path, comment = "#",
                  col_names = c("year", "month", "decimal_date", 
                                "average", "interpolated", "trend",
                                "stdev_days", "uncertainty_average"),
                  col_types = c("iidddidd"),
                  na = c("-1", "-99.99", "-9.99", "-0.99"))
co2
```

Success! We have read in the data. Celebrate!

### Plotting Data with `ggplot`

Effective visualizations are an integral part of data science, poorly organized or poorly labelled figures can be as much a source of peril as understanding.  Nevertheless, the ability to generate plots quickly with minimal tinkering is an essential skill.  As standards for visualizations have increased, too often visualization is seen as an ends rather than a means of data analysis. See [Fox & Hendler (2011)](http://science.sciencemag.org/content/331/6018/705.short) for more discussion of this.

```{r}
ggplot(co2, aes(decimal_date, average)) + geom_line()
```

**Alternative: Plotting Data with base R (no packages needed)**
If you want to plot something quickly without loading any packages. 

```{r}
plot(y=co2$average, x=co2$decimal_date, type="l",
     ylab="Average Co2", xlab="Decimal date")
```

But... it isn't quite as pretty as with ggplot. :/ 

**Plotting multiple series**

We often would like to plot several data values together for comparison,
for example the average, interpolated and trend $CO_2$ data. We can do
this in three steps:

1) subsetting the dataset to the columns desired for plotting

```{r}
co2_sub <- co2 %>%
    select(decimal_date, average, interpolated, trend)
co2_sub %>% head()
```


2) rearranging the data into a "long" data table where the data values
are stacked together in one column and there is a separate column that
keeps track of whether the data came from the average,
interpolated, or trend column. Notice by using the same name,
we overwrite the original co2_sub


```{r}
co2_sub <- co2_sub %>%
    pivot_longer(!decimal_date, names_to="series",
                 values_to="ppmv")
head(co2_sub)
```

3) Visualize the data using a line plot

```{r}
co2_sub %>%
 ggplot(aes(decimal_date, ppmv, col = series)) +
  geom_line()
```

Or we can take advantage of dplyr's nifty pipping abilities and
accomplish all of these steps in one block of code. Beyond being more
succinct, this has the added benefit of avoiding creating a new object
for the subsetted data.

```{r fig.height=3}
co2 %>%
  select(decimal_date, average, interpolated, trend) %>%
  gather(series, ppmv, -decimal_date) %>%
  ggplot(aes(decimal_date, ppmv, col = series)) +  geom_line()
```

**What do we see?**

Our "Figure 1" shows three broad patterns:

- A trend of steadily increasing CO2 concentration from 1950 to 2015
- Small, regular seasonal oscillation is visible in the data
- Increase appears to be accelerating (convex curve)

**Understanding moving averages**

**Trend, cycle, or noise?**

> "Climate is what you expect, weather is what you get"

Present-day climate data is often sampled at both finer temporal and spatial scales than we might be interested in when exploring long-term trends.  More frequent sampling can reveal higher-frequency trends, such as the seasonal pattern we observe in the CO2 record.  It can also reveal somewhat greater variability, picking up more random (stochastic) sources of variation such as weather patterns.

To reveal long term trends it is frequently valuable to average out this high-frequency variation.  We could spend the whole course discussing ways such averaging or smoothing can be done, but instead we'll focus on the most common methods you will see already present in the climate data we examine.  The monthly record data we analyze here already shows some averaging.  How was this performed?

**Moving averages**

```{r}
co2 %>%
 mutate(annual = RcppRoll::roll_mean(average,
                                     n=12L,
                                     align = "left",
                                     fill = NA,
                                     na.rm=TRUE,
                                     normalize=FALSE)) ->
  co2
head(co2)
```

```{r}
co2 %>% ggplot(aes(decimal_date)) +
  geom_line(aes(y=average), col="blue") +
  geom_line(aes(y=annual), col="red")
```

### Lab 2 Questions: Exploring and visualizing more data realted to global climate change. 

#### Question 1:

Each of the last years has consecutively set new records on global climate.  In this section we will analyze global mean temperature data. [Data from](http://climate.nasa.gov/vital-signs/global-temperature)

- 1a. Describe the data set. You can do additional searches to learn more about the data given the description on the data page is not detailed. 
ANSWER: The data set gives the average annual temperature anomaly and the local regression of the averages across 5 years.

- 1b. Describe what kind of column each data contains and what units it is measured in. 
ANSWER: The first column is time in years, the second is the average temperature anomaly in celsius, and the third is the 5 year loess smoothing also in celsius.

Then address our three key questions in understanding this data:
- 1c. How are the measurements made? What is the associated measurement uncertainty?
ANSWER: "The data are primarily collected from sources on Earth's surface, including weather stations, ships, and ocean buoys." is how they describe the collection process.
There is no defined amount of uncertainty in the measurements.

- 1d.  What is the resolution of the data?
ANSWER: The resolution of the data is by year.

- 1e. Are their missing values? How should they be handled?
ANSWER: There does not appear to be any missing values in the data.

#### Question 2:

- 2a: Construct the necessary R code to import and prepare for manipulation the [following data set](http://climate.nasa.gov/system/internal_resources/details/original/647_Global_Temperature_Data_File.txt) You may wish to re-label the columns to something easier to read/understand. 

```{r}
temp <- read_table("https://data.giss.nasa.gov/gistemp/graphs/graph_data/Global_Mean_Estimates_based_on_Land_and_Ocean_Data/graph.txt",
                   col_types = "idd", skip = 5, na = "*", col_names = c("Year", "Annual_Mean", "5-year_Mean"))
temp
```

#### Question 3:

- 3a. Plot the trend in global mean temperatures over time.  
- 3b. Describe what you see in the plot and how you interpret the patterns you observe.

```{r}
ggplot(temp, aes(x = Year, y = Annual_Mean)) +
    geom_line() +
    theme_bw()
```

ANSWER: The change in global surface temperature began to increase in the era directly after WW2, 
with the change exponentially increasing in the late 1970s.

#### Question 4: Evaluating the evidence for a "Pause" in warming?

The [2013 IPCC Report](https://www.ipcc.ch/pdf/assessment-report/ar5/wg1/WG1AR5_SummaryVolume_FINAL.pdf) included a tentative observation of a "much smaller increasing trend" in global mean temperatures since 1998 than was observed previously.  This led to much discussion in the media about the existence of a "Pause" or "Hiatus" in global warming rates, as well as much research looking into where the extra heat could have gone.  (Examples discussing this question include articles in [The Guardian](http://www.theguardian.com/environment/2015/jun/04/global-warming-hasnt-paused-study-finds), [BBC News](http://www.bbc.com/news/science-environment-28870988), and [Wikipedia](https://en.wikipedia.org/wiki/Global_warming_hiatus)). 

- 4a. By examining the data here, what evidence do you find or not find for such a pause?  Present an written/graphical analysis of this data (using the tools & methods we have covered so far) to argue your case.  

```{r}
# Take data after 1998
data_after_1998 <- temp %>%
    subset(Year >= 1998 & Year <= 2013) %>%
    mutate(YearBin = "after") %>%
    arrange(Year) %>%
    mutate(YearNum = 1:nrow(.))

# Take data from years before 1998
data_before_1998 <- temp %>%
    subset(Year < 1998 & Year >= 1982) %>%
    mutate(YearBin = "before") %>%
    arrange(Year) %>%
    mutate(YearNum = 1:nrow(.))

new_temp <- rbind(data_before_1998, data_after_1998)

ggplot(new_temp, aes(x = YearNum, color = YearBin)) +
    geom_line(aes(y = Annual_Mean))

standardized_after <- data_after_1998 %>%
    mutate(standardized_values = (Annual_Mean - mean(Annual_Mean)) / sd(Annual_Mean))

standardized_before <- data_before_1998 %>%
    mutate(standardized_values = (Annual_Mean - mean(Annual_Mean)) / sd(Annual_Mean))

standardized_temp <- rbind(standardized_before, standardized_after)

ggplot(standardized_temp, aes(x = YearNum, color = YearBin)) +
    geom_line(aes(y = standardized_values))
```

ANSWER: When isolating the years before 1998 and the years after 1998, we can see that the overall 
change in annual means does not provide evidence for a pause in global warming rates and that the rate 
has either stayed the same or increased since then.

- 4b. What additional analyses or data sources would better help you refine your arguments?

ANSWER: International data could help refine arguments and looking at the whole data set provides enough evidence as is 
to show that the change has only increased since the late 20th century.

#### Question 5: Rolling averages
    
- 5a. What is the meaning of "5 year average" vs "annual average"?
ANSWER: 5 year average is a rolling average across 5 years in the data and the annual average is the average of that one year alone.

- 5b. Construct 5 year averages from the annual data.  Construct 10 & 20-year averages. Plot the different averages and describe what differences you see and why.  

```{r}
averages <- temp %>%
    mutate(`average_5_year` = RcppRoll::roll_mean(Annual_Mean,
                                        n = 5L,
                                        align = "center",
                                        fill = NA,
                                        na.rm = TRUE,
                                        normalize = FALSE),
           `average_10_year` = RcppRoll::roll_mean(Annual_Mean,
                                        n = 10L,
                                        align = "center",
                                        fill = NA,
                                        na.rm = TRUE,
                                        normalize = FALSE),
           `average_20_year` = RcppRoll::roll_mean(Annual_Mean,
                                        n = 20L,
                                        align = "center",
                                        fill = NA,
                                        na.rm = TRUE,
                                        normalize = FALSE)) %>%
    pivot_longer(cols = starts_with("average"))
```


```{r}
ggplot(averages, aes(x = Year, y = value)) +
    geom_line() +
    facet_wrap(vars(name))
```

ANSWER: Increasing the number of years creates a much smoother line as fewer points are being included in the average.

#### Question 6: Melting Ice Sheets?

- [Data description](http://climate.nasa.gov/vital-signs/land-ice/)
- [Raw data file](http://climate.nasa.gov/system/internal_resources/details/original/499_GRN_ANT_mass_changes.csv)

- 6a. Describe the data set: what are the columns and units? Where do the numbers come from? 

ANSWER: The first column gives the year in decimal form,
the second gives the mass of greenland in gigatonnes compared to the timeseries mean, 
and the third gives the same as the second but for antarctica. The data comes from NASA satellites.

- 6b.  What is the uncertainty in measurement? Resolution of the data? Interpretation of missing values?

ANSWER: 

- Uncertainty: There does not appear to be any uncertainty in the measurement
- Resolution: Years in decimal form
- Missing data: There are no missing values in the dataset

- 6c. Construct the necessary R code to import this data set as a tidy `Table` object.
```{r}
### check your working directory!
### your code
ice_data <- read_csv("data/499_GRN_ANT_mass_changes.csv", skip = 9)

colnames(ice_data) <- c("year", "greenland_mass", "antarctica_mass")

head(ice_data)
```

- 6d: Plot the data and describe the trends you observe.

```{r}
ggplot(ice_data, aes(x = year, y = greenland_mass)) +
    geom_line()

ggplot(ice_data, aes(x = year, y = antarctica_mass)) +
    geom_line()
```
ANSWER: Both antarctica and greenland have decreased in mass over time. This relationship is roughly linear 
and for each year the mass of each increases slightly during the winter and quickly decreases during the summer.

Isn't this fun?! 

#### Question 7: Rising Sea Levels?

- [Data description](http://climate.nasa.gov/vital-signs/sea-level/)
- [Raw data file](http://climate.nasa.gov/system/internal_resources/details/original/121_Global_Sea_Level_Data_File.txt)

- 7a. Describe the data set: what are the columns and units? 
ANSWER: The first column is the year and fraction of the year, the second column is the global mean sea level in centimeters,
the third column is the global mean sea level in centimeters with 60 day smoothing applied

- 7b. Where do these data come from (try googling if the description isn't satisfactory)? 
ANSWER: This data is taken from NASA satellite missions (TOPEX/Poseidon, Jason series, and sentinel-6)

- 7c. What is the uncertainty in measurement? Resolution of the data? Interpretation of missing values?

ANSWER:  

- Uncertainty: There is no defined uncertainty in the data.
- Resolution: The resolution is year + fraction of the year.
- Missing data: The header says that missing values are coded as "9.96921E36f", but this value does not show up in the dataset.

#### Question 8:

- 8a. Construct the necessary R code to import this data set as a tidy `Table` object.
```{r}
### your code
sea_data <- read_table("data/sealevel.txt", comment = "HDR",
                       col_names = c("year", "gmsl", "gmsl_smooth"))
```

- 8b. Plot the data.

```{r}
ggplot(sea_data, aes(x = year, y = gmsl)) +
    geom_line()

ggplot(sea_data, aes(x = year, y = gmsl_smooth)) +
    geom_line()
```

- 8c. Describe the trends you observe.

ANSWER: The global mean sea level has increased over time to just over 9 cm since 1993.
This relationship has been roughly linear consistently since the data was collected.

### Demo: Exploring Seasonal Oscillations

Circling back to our CO2 data from Mauna Loa, let's practice summarizing our data in different ways. 

- Demonstrate that the periodic behavior truly is seasonal
- What month is the maximum? What is the minimum?
- What do you think could explain the seasonal cycle observed here?

```{r}
### Seasonal oscillation
co2 %>%
  group_by(year) %>%
  ### Note we need month[which.max] since not all years have all 12 months
  summarise(max_month = month[which.max(average)],
            min_month = month[which.min(average)]) %>%
  gather(id, value, -year) %>%
  ggplot(aes(value, fill=id)) + stat_count()
```

### Question 9 (Gradaute student or Extra credit) : Summarize the data of your choice. 

Using any of the datasets from this demo/lab (co2, icesheets, temp etc.) come up with an interesting question to ask that necessitates you to summarize the data in some way, make a visualization, and answer your question. 

- 9a. What dataset are you using? What is your question?

- 9b. Summarize your data to answer your question. 

```{r}

```

- 9c. Plot your data
```{r}

```

- 9d. Interpret you plot. What is the answer to your question?
ANSWER: ...